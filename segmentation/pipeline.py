# =====================================================
# SEGMENTATION PIPELINE - ORCHESTRATEUR PRINCIPAL
# =====================================================
# Pipeline intelligent style ChatGPT
# USER PROMPT ‚Üí INTENT ‚Üí TARGET ‚Üí SEMANTIC ‚Üí INSTANCE ‚Üí FUSION ‚Üí VALIDATION

import torch
import numpy as np
from PIL import Image
from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass

from .intent_parser import parse_intent, Intent
from .target_resolver import resolve_target, Target
from .semantic_segmentation import (
    semantic_segment,
    SemanticMap,
    get_combined_mask,
    subtract_masks
)
from .instance_segmentation import (
    instance_segment_with_points,
    instance_segment_from_semantic,
    sample_points_from_mask
)
from .mask_fusion import fuse_masks, MaskLayers
from .mask_refinement import refine_mask, get_dynamic_refinement_params
from .validation import (
    validate_mask,
    auto_correct_mask,
    ValidationResult,
    ValidationStatus
)


@dataclass
class SegmentationResult:
    """R√©sultat final de la segmentation"""
    
    # Masque final
    final_mask: Image.Image
    
    # Masques interm√©diaires
    target_mask: Optional[Image.Image] = None
    protected_mask: Optional[Image.Image] = None
    context_mask: Optional[Image.Image] = None
    
    # ‚ú® NOUVEAU: Masques de transition pour blending
    transition_masks: Optional[Any] = None  # TransitionMasks
    
    # M√©tadonn√©es
    intent: Optional[Intent] = None
    target: Optional[Target] = None
    semantic_map: Optional[SemanticMap] = None
    validation: Optional[ValidationResult] = None
    
    # Stats
    coverage: float = 0.0
    processing_time: float = 0.0


# =====================================================
# PIPELINE PRINCIPAL
# =====================================================

def segment_from_prompt(
    image: Image.Image,
    user_prompt: str,
    sam2_predictor: Optional[Any] = None,
    segformer_model: Optional[Any] = None,
    segformer_processor: Optional[Any] = None,
    device: str = "cuda",
    auto_correct: bool = True,
    refine_target_with_sam2: bool = False,
    verbose: bool = True
) -> SegmentationResult:
    """
    Pipeline de segmentation intelligent
    
    √âTAPES:
    1. Parse l'intention du prompt
    2. R√©sout les cibles (primary/protected/context)
    3. Segmentation s√©mantique (OneFormer/SegFormer)
    4. Segmentation instance (SAM2) - OPTIONNEL
    5. Fusion des masques avec priorit√©s
    6. Raffinement du masque
    7. Validation et auto-correction
    
    Args:
        image: Image PIL √† segmenter
        user_prompt: Prompt utilisateur (ex: "change the facade to white modern")
        sam2_predictor: Mod√®le SAM2 pr√©-charg√©
        segformer_model: Mod√®le SegFormer pr√©-charg√©
        segformer_processor: Processor SegFormer pr√©-charg√©
        device: Device (cuda/cpu)
        auto_correct: Tenter l'auto-correction si masque invalide
        refine_target_with_sam2: Si True, raffine UNIQUEMENT le target avec SAM2
        verbose: Afficher les logs
    
    Returns:
        SegmentationResult avec le masque final et m√©tadonn√©es
    """
    
    import time
    start_time = time.time()
    
    if verbose:
        print("=" * 60)
        print("üéØ SEGMENTATION PIPELINE")
        print("=" * 60)
        print(f"üìù Prompt: \"{user_prompt}\"")
        print(f"üìê Image: {image.size}")
        print()
    
    # =========================================================
    # √âTAPE 1: PARSE L'INTENTION
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 1: INTENT PARSING")
        print("‚îÅ" * 40)
    
    intent = parse_intent(user_prompt)
    
    if verbose:
        print(f"   Action: {intent.action}")
        print(f"   Action Type: {intent.action_type}")  # ‚ú® NOUVEAU
        print(f"   Target: {intent.target_hint}")
        if intent.action_type == "ADD":
            print(f"   Object to Add: {intent.object_to_add}")
            print(f"   Location: {intent.location}")
        print(f"   Material: {intent.material}")
        print(f"   Color: {intent.color}")
        print(f"   Style: {intent.style}")
        print()
    
    # =========================================================
    # √âTAPE 2: R√âSOLUTION DES CIBLES
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 2: TARGET RESOLUTION")
        print("‚îÅ" * 40)
    
    target = resolve_target(intent)
    
    if verbose:
        print(f"   Primary: {target.primary}")
        print(f"   Protected: {target.protected}")
        print(f"   Context: {target.context}")
        print()
    
    # =========================================================
    # √âTAPE 3: SEGMENTATION S√âMANTIQUE
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 3: SEMANTIC SEGMENTATION")
        print("‚îÅ" * 40)
    
    semantic_map = semantic_segment(
        image=image,
        model_type="oneformer"  # Utiliser OneFormer par d√©faut
    )
    
    if verbose:
        detected_classes = list(semantic_map.masks.keys())[:10]
        print(f"   ‚úÖ Classes d√©tect√©es: {', '.join(detected_classes)}")
        print()    
    # =========================================================
    # √âTAPE 3.5: SPATIAL ZONE DETECTION (pour actions ADD)
    # =========================================================
    
    spatial_zone = None
    depth_map = None
    
    if intent.action_type == "ADD":
        if verbose:
            print("‚îÄ" * 40)
            print("√âTAPE 3.5: SPATIAL ZONE DETECTION (ADD)")
            print("‚îÄ" * 40)
        
        # G√©n√©rer depth map pour d√©tection de zones
        try:
            from steps.step2_preprocess import make_depth
            depth_pil = make_depth(image, save_path="output/depth_map.png")
            depth_map = np.array(depth_pil)
            
            if verbose:
                print(f"   ‚úÖ Depth map g√©n√©r√©e")
        except Exception as e:
            if verbose:
                print(f"   ‚ö†Ô∏è  Depth map non disponible: {e}")
            depth_map = None
        
        # D√©tecter la zone spatiale
        from .spatial_zones import detect_spatial_zone
        
        zone_description = intent.location or "ground_foreground"
        
        spatial_zone = detect_spatial_zone(
            image=image,
            zone_description=zone_description,
            semantic_masks=semantic_map.masks,
            depth_map=depth_map
        )
        
        if verbose:
            from .spatial_zones import describe_zone
            print(f"   ‚úÖ Zone d√©tect√©e: {describe_zone(spatial_zone)}")
            
            # Sauvegarder preview de la zone
            from .spatial_zones import visualize_zone
            zone_preview = visualize_zone(image, spatial_zone, alpha=0.5)
            zone_preview.save("output/spatial_zone_preview.png")
            print(f"   ‚úÖ Preview: output/spatial_zone_preview.png")
        
        print()    
    # =========================================================
    # √âTAPE 4: INSTANCE SEGMENTATION (SAM2) - OPTIONNEL
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 4: INSTANCE SEGMENTATION (OPTIONNEL)")
        print("‚îÅ" * 40)
    
    # Pour l'instant, on ne fait pas de segmentation d'instance s√©par√©e
    # SAM2 sera utilis√© dans l'√©tape de fusion si refine_target_with_sam2=True
    
    if verbose:
        if refine_target_with_sam2:
            print(f"   ‚úÖ Raffinement SAM2 activ√© (sera appliqu√© au target uniquement)")
        else:
            print(f"   ‚ÑπÔ∏è  Raffinement SAM2 d√©sactiv√©")
    
    print()
    
    # =========================================================
    # √âTAPE 5: FUSION DES MASQUES
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        if intent.action_type == "ADD":
            print("√âTAPE 5: ADDITIVE MASK CREATION")
        else:
            print("√âTAPE 5: MASK FUSION + SAM2 REFINEMENT")
        print("‚îÅ" * 40)
    
    # NOUVEAU: Distinction ADD vs MODIFY
    if intent.action_type == "ADD" and spatial_zone:
        # MODE ADDITIF: Utiliser la zone spatiale comme masque
        if verbose:
            print(f"   ‚ú® Mode ADDITIF: Masque = zone d'accueil")
            print(f"   ‚ùå Pas de remplacement du contenu existant")
        
        # Le masque final = zone spatiale (avec protection int√©gr√©e)
        final_mask_pil = spatial_zone.mask
        
        # Cr√©er mask_layers compatible
        from .mask_fusion import MaskLayers
        mask_layers = MaskLayers(
            target=final_mask_pil,
            protected=Image.new("L", image.size, 0),  # Pas de protection suppl√©mentaire
            context=Image.new("L", image.size, 0),
            final=final_mask_pil
        )
        
        primary_semantic_mask = final_mask_pil
        
    else:
        # MODE CLASSIQUE: Fusionner avec les priorit√©s
        # SAM2 sera appliqu√© au target uniquement si refine_target_with_sam2=True
        # Grounding DINO sera utilis√© pour les ouvertures si manquantes
        mask_layers = fuse_masks(
            semantic_map=semantic_map,
            target=target,
            refine_target_with_sam2=refine_target_with_sam2,
            use_grounding_dino_for_protected=True,  # Approche hybride activ√©e
            original_image=image
        )
        
        # Pour auto-correction
        primary_semantic_mask = get_combined_mask(semantic_map, target.primary)
    
    if verbose:
        print()
    
    # =========================================================
    # √âTAPE 6: RAFFINEMENT DU MASQUE
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 6: MASK REFINEMENT")
        print("‚îÅ" * 40)
    
    # Param√®tres dynamiques
    refinement_params = get_dynamic_refinement_params(
        mask_layers.final,
        image.size
    )
    
    refined_mask = refine_mask(
        mask_layers.final,
        **refinement_params
    )
    
    if verbose:
        print()
    
    # =========================================================
    # √âTAPE 6.5: CR√âATION DES MASQUES DE TRANSITION
    # =========================================================
    
    transition_masks = None
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 6.5: TRANSITION MASKS (BLENDING)")
        print("‚îÅ" * 40)
    
    try:
        from .transition_masks import (
            create_transition_masks,
            compute_adaptive_transition_width,
            visualize_transition_masks,
            create_mask_comparison
        )
        
        # Calculer largeur adaptative
        transition_width = compute_adaptive_transition_width(
            refined_mask,
            image.size,
            base_width=12
        )
        
        # Cr√©er masques de transition
        transition_masks = create_transition_masks(
            mask_core=refined_mask,
            transition_width=transition_width,
            gradient_type="cosine",  # Plus doux que lin√©aire
            adaptive_feather=True  # ‚ú® Feathering adaptatif bas√© sur aire du masque
        )
        
        if verbose:
            print(f"   ‚úÖ Transition width: {transition_width}px")
            print(f"   ‚úÖ Gradient type: cosine")
            
            # Sauvegarder visualisations
            visualize_transition_masks(
                image,
                transition_masks,
                save_path="output/transition_preview.png"
            )
            
            create_mask_comparison(
                transition_masks,
                save_path="output/transition_masks_comparison.png"
            )
            
            # Sauvegarder masques individuels
            transition_masks.core.save("output/mask_core.png")
            transition_masks.transition.save("output/mask_transition.png")
            transition_masks.combined.save("output/mask_combined.png")
            
            print(f"   üíæ Preview: output/transition_preview.png")
            print(f"   üíæ Comparison: output/transition_masks_comparison.png")
    
    except Exception as e:
        if verbose:
            print(f"   ‚ö†Ô∏è  Transition masks non cr√©√©s: {e}")
        transition_masks = None
    
    if verbose:
        print()
    
    # =========================================================
    # √âTAPE 7: VALIDATION
    # =========================================================
    
    if verbose:
        print("‚îÅ" * 40)
        print("√âTAPE 7: VALIDATION")
        print("‚îÅ" * 40)
    
    validation_result = validate_mask(refined_mask)
    
    # Auto-correction si n√©cessaire
    if not validation_result.is_valid and auto_correct:
        validation_result = auto_correct_mask(
            mask=refined_mask,
            validation_result=validation_result,
            semantic_mask=primary_semantic_mask
        )
        refined_mask = validation_result.mask
    
    if verbose:
        print()
    
    # =========================================================
    # R√âSULTAT FINAL
    # =========================================================
    
    processing_time = time.time() - start_time
    
    # Calculer la couverture
    mask_array = np.array(refined_mask)
    coverage = np.sum(mask_array > 127) / mask_array.size
    
    if verbose:
        print("=" * 60)
        print("‚úÖ SEGMENTATION TERMIN√âE")
        print("=" * 60)
        print(f"   Couverture: {coverage:.1%}")
        print(f"   Temps: {processing_time:.2f}s")
        print(f"   Status: {validation_result.status.value}")
        print()
    
    return SegmentationResult(
        final_mask=refined_mask,
        target_mask=mask_layers.target,
        protected_mask=mask_layers.protected,
        context_mask=mask_layers.context,
        transition_masks=transition_masks,  # ‚ú® NOUVEAU
        intent=intent,
        target=target,
        semantic_map=semantic_map,
        validation=validation_result,
        coverage=coverage,
        processing_time=processing_time
    )


# =====================================================
# FONCTIONS SIMPLIFI√âES
# =====================================================

def quick_segment(
    image: Image.Image,
    target_classes: list,
    protected_classes: list = None,
    segformer_model: Optional[Any] = None,
    segformer_processor: Optional[Any] = None,
    device: str = "cuda"
) -> Image.Image:
    """
    Segmentation rapide sans parsing de prompt
    
    Args:
        image: Image √† segmenter
        target_classes: Classes √† cibler (ex: ["floor", "rug"])
        protected_classes: Classes √† prot√©ger (ex: ["person", "furniture"])
    
    Returns:
        Masque final
    """
    
    # Segmentation s√©mantique
    semantic_map = semantic_segment(
        image=image,
        model=segformer_model,
        processor=segformer_processor,
        device=device
    )
    
    # Masque target
    target_mask = get_combined_mask(semantic_map, target_classes)
    
    # Masque protected
    if protected_classes:
        protected_mask = get_combined_mask(semantic_map, protected_classes)
    else:
        protected_mask = None
    
    # Fusion
    mask_layers = fuse_masks(
        target_mask=target_mask,
        protected_mask=protected_mask
    )
    
    # Raffinement
    refined = refine_mask(mask_layers.final)
    
    return refined


def segment_element(
    image: Image.Image,
    element: str,
    segformer_model: Optional[Any] = None,
    segformer_processor: Optional[Any] = None,
    device: str = "cuda"
) -> Image.Image:
    """
    Segmente un √©l√©ment sp√©cifique
    
    Args:
        image: Image √† segmenter
        element: √âl√©ment √† cibler ("floor", "wall", "ceiling", etc.)
    
    Returns:
        Masque de l'√©l√©ment
    """
    
    # Mapping simple
    ELEMENT_CLASSES = {
        "floor": ["floor", "rug", "carpet"],
        "wall": ["wall"],
        "ceiling": ["ceiling"],
        "furniture": ["sofa", "chair", "table", "bed", "cabinet"],
        "window": ["window", "windowpane"],
        "door": ["door"],
        "light": ["lamp", "chandelier", "light"],
        "plant": ["plant", "tree", "flower"],
        "art": ["painting", "poster"],
        "rug": ["rug", "carpet", "mat"]
    }
    
    target_classes = ELEMENT_CLASSES.get(element, [element])
    
    return quick_segment(
        image=image,
        target_classes=target_classes,
        segformer_model=segformer_model,
        segformer_processor=segformer_processor,
        device=device
    )


# =====================================================
# CHARGEMENT DES MOD√àLES
# =====================================================

def load_segmentation_models(device: str = "cuda") -> dict:
    """
    Charge tous les mod√®les n√©cessaires pour la segmentation
    
    Returns:
        Dict avec segformer_model, segformer_processor, sam2_predictor
    """
    
    print("üîÑ Chargement des mod√®les de segmentation...")
    
    models = {}
    
    # SegFormer
    try:
        from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor
        
        models["segformer_processor"] = SegformerImageProcessor.from_pretrained(
            "nvidia/segformer-b5-finetuned-ade-640-640"
        )
        models["segformer_model"] = SegformerForSemanticSegmentation.from_pretrained(
            "nvidia/segformer-b5-finetuned-ade-640-640"
        ).to(device)
        
        print("   ‚úì SegFormer charg√©")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur SegFormer: {e}")
        models["segformer_model"] = None
        models["segformer_processor"] = None
    
    # SAM2
    try:
        from sam2.sam2_image_predictor import SAM2ImagePredictor
        
        models["sam2_predictor"] = SAM2ImagePredictor.from_pretrained(
            "facebook/sam2-hiera-large"
        )
        
        print("   ‚úì SAM2 charg√©")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur SAM2: {e}")
        models["sam2_predictor"] = None
    
    print("‚úÖ Mod√®les de segmentation pr√™ts")
    
    return models
